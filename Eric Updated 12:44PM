# Load essential libraries
library(data.table)  # For efficient data loading
library(tm)          # For text cleaning and preprocessing
library(Matrix) # For sparse matrix handling
library(irlba)       # For dimensionality reduction
library(dplyr)       # For data manipulation
library(ggplot2)     # For visualizations
library(LaF)
rm(list = ls())  # Remove all objects
gc()             # Force garbage collection


# Load a manageable chunk of the dataset (random 100,000 rows)------------------

#Define file path and column types
file_path <- "C:/Users/erics/Downloads/song_lyrics.csv"

column_types <- c(
  "string",  # Column 1
  "string",  # Column 2
  "string",  # Column 3
  "integer", # Column 4
  "integer", # Column 5
  "string",  # Column 6
  "string",  # Column 7
  "integer", # Column 8
  "string",  # Column 9
  "string",  # Column 10
  "string"   # Column 11
)

# Step 1: Read the header line to get column names
header_line <- readLines(file_path, n = 1)
column_names <- strsplit(header_line, split = ",")[[1]]

# Verify that the number of column types matches the number of columns
if (length(column_types) != length(column_names)) {
  stop("The number of column types does not match the number of columns in the CSV.")
}

# Step 2: Open the CSV file with LaF, skipping the header
laf <- laf_open_csv(
  filename = file_path,
  column_types = column_types,
  column_names = column_names,
  sep = ",",
  skip = 1  # Skip the header row
)

# Step 3: Determine total number of rows
total_rows <- 5063837
cat("Total number of rows in the dataset:", total_rows, "\n")

# Step 4: Define sample size and generate random indices
sample_size <- 100000

# Check if the sample size is feasible
if (sample_size > total_rows) {
  stop("Sample size exceeds the total number of rows in the dataset.")
}

# Set seed for reproducibility
set.seed(123)

# Generate random row indices without replacement
sample_indices <- sort(sample(total_rows, sample_size, replace = FALSE))

# Step 5: Read the sampled rows
sampled_data <- laf[sample_indices, ]

# Convert to a data.frame for easier manipulation (optional)
sampled_df <- as.data.frame(sampled_data)

# Display the first few rows of the sampled data
head(sampled_df)

# Step 6: Inspect the data structure
str(sampled_df)
summary(sampled_df)


# ----- DATA CLEANING -----
# Log-transform the target variable
sampled_data$log_views <- log1p(sampled_data$views)  # log1p handles log(0)

# Create a text corpus from the 'lyrics' column
corpus <- Corpus(VectorSource(sampled_data$lyrics))

# Preprocess text: Lowercase, remove punctuation, numbers, and stopwords
corpus <- tm_map(corpus, content_transformer(tolower))  # Convert to lowercase
corpus <- tm_map(corpus, removePunctuation)            # Remove punctuation
corpus <- tm_map(corpus, removeNumbers)                # Remove numbers
corpus <- tm_map(corpus, removeWords, stopwords("en")) # Remove common English stopwords

# ----- Exploratory Data Analysis (EDA) -----

# Original views distribution
ggplot(sampled_data, aes(x = views)) +
  geom_histogram(bins = 30, fill = "blue", color = "black") +
  labs(title = "Distribution of Song Views", x = "Views", y = "Count")

# Log-transformed views distribution
ggplot(sampled_data, aes(x = log_views)) +
  geom_histogram(bins = 30, fill = "blue", color = "black") +
  labs(title = "Distribution of Log-Transformed Song Views", x = "Log(Views)", y = "Count")

ggplot(sampled_data, aes(x = "", y = views)) +
  geom_boxplot() +
  labs(title = "Boxplot of Song Views", x = "All Songs", y = "Views")

# ----- FEATURE ENGINEERING -----
# 1. Genre (tag): One-hot encoding
genre_dummies <- dummyVars(~ tag, data = sampled_data)
genre_encoded <- predict(genre_dummies, sampled_data)
sampled_data <- cbind(sampled_data, genre_encoded)

# 2. Release Year and Song Age
sampled_data$song_age <- 2024 - sampled_data$year  # Drop 'year' later if collinearity exists

# 3. Number of Collaborators
sampled_data$num_collaborators <- sapply(strsplit(sampled_data$features, ","), length)

# 4. Word Count
sampled_data$word_count <- sapply(strsplit(sampled_data$lyrics, "\\s+"), length)

# 5. Sentiment Analysis
tokenized_lyrics <- sampled_data %>% unnest_tokens(word, lyrics)
sentiment_scores <- tokenized_lyrics %>%
  inner_join(get_sentiments("afinn"), by = "word") %>%
  group_by(id) %>%
  summarise(sentiment = sum(value, na.rm = TRUE))
sampled_data <- left_join(sampled_data, sentiment_scores, by = "id")
sampled_data$sentiment[is.na(sampled_data$sentiment)] <- 0  # Replace missing sentiment with 0

# 6. Interaction Features
sampled_data$word_sentiment_interaction <- sampled_data$word_count * sampled_data$sentiment
sampled_data$collab_sentiment_interaction <- sampled_data$num_collaborators * sampled_data$sentiment

# 7. Normalize Features
numeric_features <- c("song_age", "num_collaborators", "word_count", "sentiment", 
                      "word_sentiment_interaction", "collab_sentiment_interaction")
# Scale numeric features in-place
sampled_data[, (numeric_features) := lapply(.SD, scale), .SDcols = numeric_features]

# ----- MODEL DATASET -----
# Prepare final dataset for modeling
model_data <- sampled_data %>%
  select(log_views, song_age, num_collaborators, word_count, sentiment, 
         word_sentiment_interaction, collab_sentiment_interaction, starts_with("tag_"))

# ----- TRAIN-TEST SPLIT -----
train_indices <- sample(1:nrow(model_data), size = 0.7 * nrow(model_data))
train_data <- model_data[train_indices, ]
test_data <- model_data[-train_indices, ]

# ----- BASELINE MODEL: LINEAR REGRESSION -----
lm_model <- lm(log_views ~ ., data = train_data)
summary(lm_model)

# Evaluate Linear Regression
lm_predictions <- predict(lm_model, test_data)
lm_rmse <- rmse(test_data$log_views, lm_predictions)
cat("Linear Regression RMSE:", lm_rmse, "\n")

# ----- ADVANCED MODEL: RANDOM FOREST -----
rf_model <- randomForest(log_views ~ ., data = train_data, ntree = 500, mtry = 3, importance = TRUE)
rf_predictions <- predict(rf_model, test_data)
rf_rmse <- rmse(test_data$log_views, rf_predictions)
cat("Random Forest RMSE:", rf_rmse, "\n")

# ----- ADVANCED MODEL: XGBOOST -----
train_matrix <- as.matrix(train_data[, -1])  # Exclude target variable
test_matrix <- as.matrix(test_data[, -1])
dtrain <- xgb.DMatrix(data = train_matrix, label = train_data$log_views)
dtest <- xgb.DMatrix(data = test_matrix)

xgb_params <- list(
  objective = "reg:squarederror",
  eta = 0.1,  # Learning rate
  max_depth = 6,
  subsample = 0.8,
  colsample_bytree = 0.8
)

xgb_model <- xgboost(data = dtrain, params = xgb_params, nrounds = 100)
xgb_predictions <- predict(xgb_model, dtest)
xgb_rmse <- rmse(test_data$log_views, xgb_predictions)
cat("XGBoost RMSE:", xgb_rmse, "\n")

# ----- ORIGINAL SCALE EVALUATION -----
rf_predictions_original <- expm1(rf_predictions)
rf_rmse_original <- rmse(expm1(test_data$log_views), rf_predictions_original)
cat("Random Forest RMSE (Original Scale):", rf_rmse_original, "\n")

xgb_predictions_original <- expm1(xgb_predictions)
xgb_rmse_original <- rmse(expm1(test_data$log_views), xgb_predictions_original)
cat("XGBoost RMSE (Original Scale):", xgb_rmse_original, "\n")

# ----- VISUALIZATIONS -----
# 1. Actual vs. Predicted Scatter Plots
ggplot(data = data.frame(actual = expm1(test_data$log_views), predicted = rf_predictions_original),
       aes(x = actual, y = predicted)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  labs(title = "Random Forest: Actual vs. Predicted Views", x = "Actual Views", y = "Predicted Views")

ggplot(data = data.frame(actual = expm1(test_data$log_views), predicted = xgb_predictions_original),
       aes(x = actual, y = predicted)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = "blue") +
  labs(title = "XGBoost: Actual vs. Predicted Views", x = "Actual Views", y = "Predicted Views")

# 2. Feature Importance
varImpPlot(rf_model, main = "Feature Importance: Random Forest")
xgb.importance <- xgb.importance(feature_names = colnames(train_matrix), model = xgb_model)
xgb.plot.importance(xgb.importance, main = "Feature Importance: XGBoost")




